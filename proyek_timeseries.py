# -*- coding: utf-8 -*-
"""proyek_timeseries.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uUSXM-K2bYD1XEgfrCExju30ehOPd7iJ
"""

# Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
import statsmodels.api as sm
from statsmodels.tsa.statespace.sarimax import SARIMAX
import warnings
warnings.filterwarnings("ignore")

"""# **Load Data**"""

# Load dataset
traffic = pd.read_csv('Metro_Interstate_Traffic_Volume.csv',
                      index_col='date_time',
                      parse_dates=True,
                      infer_datetime_format=True,
                      na_values=[],           # tidak anggap apapun sebagai NaN
                      keep_default_na=False   # nonaktifkan behavior default-nya
)
traffic

"""# **Exploratory Data Analysis**"""

# Memeriksa informasi dataset
traffic.info()

"""**Insight**:
- Terdapat 8 variabel, dengan jumlah data sebanyak 48204
- Berikut penjelasan setiap variabel:
  - holiday: hari libur nasional (kategorik)
  - temp: suhu rata-rata dalam satuan Kelvin (numerik)
  - rain_1h: curah hujan/jam dalam satuan mm (numerik)
  - snow_1h: curah salju/jam dalam satuan mm (numerik)
  - clouds_all: persentase tutupan awan (numerik)
  - weather_main: deskripsi cuaca singkat (kategorik)
  - weather_description: deskripsi cuaca lengkap (kategorik)
  - traffic_volume: volume lalu lintas per jam dengan satuan kendaraan (numerik)
"""

# Menampilkan statistika deskriptif dataset
traffic.describe()

"""**Insight**:
- Rata-rata suhu selama pencatatan kurang lebih 6 tahun (2012-2018) adalah 281.2 Kelvin, dengan suhu terendah 0 Kelvin dan suhu tertinggi 310.07 Kelvin
- Rata-rata curah hujan perjam selama pencatatan kurang lebih 6 tahun (2012-2018) adalah 0.33 mm/jam, dengan curah hujan terendah 0 mm/jam (tidak terjadi hujan) dan tertinggi 9831.3 mm/jam (sepertinya ada kesalahan karena tidak mungkin mencapai angka tersebut)
- Rata-rata curah salju perjam selama pencatatan kurang lebih 6 tahun (2012-2018) adalah 0.0002 mm/jam, dengan curah salju terendah 0 mm/jam (tidak turun salju) dan tertinggi 0.51 mm/jam
- Rata-rata persentase tutupan awan selama pencatatan kurang lebih 6 tahun (2012-2018) adalah 49.36%, dengan tutupan awan terendah 0% (tidak ada awan) dan tutupan awan tertinggi 100%
- Rata-rata volume lalu lintas perjam selama pencatatan kurang lebih 6 tahun (2012-2018) adalah 3260 kendaraan, dengan volume terendah adalah 0 kendaraan perjam (tidak ada kendaraan) dan tertinggi 7280 kendaraan perjam
"""

# Menampilkan rata-rata lalu lintas berdasarkan hari libur
plt.figure(figsize=(10, 6))

# Hitung rata-rata traffic volume per holiday
holiday_avg = traffic.groupby('holiday')['traffic_volume'].mean().sort_values(ascending=False)

# Tentukan holiday tertinggi selain 'None'
holiday_excl_none = holiday_avg.drop('None')
top_holiday = holiday_excl_none.idxmax()

colors = ['#FF5733' if holiday == top_holiday else '#3498db' for holiday in holiday_avg.index]

# Horizontal barplot
sns.barplot(y=holiday_avg.index, x=holiday_avg.values, palette=colors)

# Judul dan label
plt.title("Rata-rata Traffic Volume per Holiday", fontsize=14)
plt.xlabel("Traffic Volume", fontsize=12)
plt.ylabel("Holiday", fontsize=12)
plt.tight_layout()
plt.show()

"""**Insight**:
Rata-rata kepadatan lalu lintas terbanyak terjadi ketika libur Tahun Baru yang mencapai hampir 1500 kendaraan setiap jam
"""

# Menampilkan distribusi volume lalu lintas
plt.figure(figsize=(10, 6))
sns.histplot(traffic['traffic_volume'], bins=50, kde=True)
plt.title('Distribusi Traffic Volume')
plt.show()

"""**Insight**:
- Distribusi volume lalu lintas bersifat multimodal, dimana terdapat beberapa puncak yang mengindikasikan waktu-waktu tertentu seperti jam sibuk pagi atau sore
- Ada lonjakan besar pada volume < 1000 kendaraan/jam, mungkin terjadi pada malam hari atau hari libur
- Volume lalu lintas tinggi jarang terjadi tetapi tetap ada
"""

# Heatmap Korelasi antar Fitur Numerik
plt.figure(figsize=(10, 6))
numerical = traffic[['temp', 'rain_1h', 'snow_1h', 'clouds_all', 'traffic_volume']]
sns.heatmap(numerical.corr(), annot=True, cmap='coolwarm')
plt.title("Korelasi Antar Fitur Numerik")
plt.tight_layout()
plt.show()

"""**Insight**:
- Korelasi antar fitur numerik sangat lemah terhadap volume lalu lintas (traffic_volume)
- Fitur suhu (temp) memiliki korelasi tertinggi dengan volume lalu lintas, sebesar 0.13 masih sangat lemah
- Curah hujan (rain_1h) dan curah salju (snow_1h) hampir tidak berkorelasi dengan volume lalu lintas
- Hal ini berarti, cuaca tidak terlalu memengaruhi volume lalu lintas, mungkin fitur waktu lebih relevan daripada faktor cuaca

# **Data Preprocessing**
"""

# Time Feature Engineering
traffic['hour'] = traffic.index.hour
traffic['day'] = traffic.index.day
traffic['month'] = traffic.index.month
traffic['year'] = traffic.index.year
traffic['weekday'] = traffic.index.weekday

"""## **Missing Values**"""

# Deteksi Missing Values
traffic.isnull().sum()

"""**Insight**:
Tidak ada missing values

## **Data Duplikat**
"""

# Deteksi Data Duplikat
print("Jumlah duplikasi: ", traffic.duplicated().sum())

# Menghapus Data Duplikat
traffic.drop_duplicates(inplace=True)

# Periksa Kembali Data Duplikat
print("Jumlah duplikasi setelah penanganan: ", traffic.duplicated().sum())

"""## **Outliers**"""

# Pilih semua kolom numerik
num_cols = traffic.select_dtypes(include=['number']).columns

# Loop untuk visualisasi dan deteksi outlier
for col in num_cols:
    Q1 = traffic[col].quantile(0.25)
    Q3 = traffic[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = traffic[(traffic[col] < lower_bound) | (traffic[col] > upper_bound)]

    print(f"\nðŸ“Š Column: {col}")
    print(f"Outlier count: {len(outliers)}")

    plt.figure(figsize=(10, 4))
    sns.boxplot(x=traffic[col])
    plt.title(f'Boxplot of {col}')
    plt.show()

"""**Insight**:
- Terdapat 3 fitur numerik yang memiliki outlier, antara lain:
  - Suhu dengan 10 outlier
  - Curah hujan dengan 3467 outlier
  - Curah salju dengan 63 outlier
"""

# Kolom numerik yang mengandung outlier
outlier_cols = ['temp', 'rain_1h', 'snow_1h']

for col in outlier_cols:
    Q1 = traffic[col].quantile(0.25)
    Q3 = traffic[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Tandai nilai outlier sebagai NaN
    traffic[col] = traffic[col].where((traffic[col] >= lower_bound) & (traffic[col] <= upper_bound), np.nan)

# Imputasi nilai outlier (sekarang NaN) dengan interpolasi berbasis waktu
traffic[outlier_cols] = traffic[outlier_cols].interpolate(method='time')

# Pastikan tidak ada NaN tersisa (misalnya di awal/akhir seri)
traffic[outlier_cols] = traffic[outlier_cols].fillna(method='bfill').fillna(method='ffill')

"""## **Feature Engineering**"""

# Lag Features
traffic['lag1'] = traffic['traffic_volume'].shift(1)
traffic['lag24'] = traffic['traffic_volume'].shift(24)
traffic['lag168'] = traffic['traffic_volume'].shift(168)

# Drop missing values akibat pembuatan lag
traffic.dropna(inplace=True)

# Cyclical Encoding
traffic['hour_sin'] = np.sin(2 * np.pi * traffic['hour']/24)
traffic['hour_cos'] = np.cos(2 * np.pi * traffic['hour']/24)
traffic['weekday_sin'] = np.sin(2 * np.pi * traffic['weekday']/7)
traffic['weekday_cos'] = np.cos(2 * np.pi * traffic['weekday']/7)

"""## **Fitur Kategorik**"""

# Drop weather_description karena kurang relevan
traffic.drop('weather_description', axis=1, inplace=True)

# One-Hot Encoding untuk holiday dan weather_main
traffic = pd.get_dummies(traffic, columns=['holiday', 'weather_main'], drop_first=True)

traffic.head()

"""# **Train-Test Split**"""

# Definisikan Fitur dan Target
X = traffic.drop(["traffic_volume"], axis=1)
y = traffic["traffic_volume"]

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 123)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""# **Standarisasi**"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# **Model Development**

## **Model XGBoost**
"""

xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6)
xgb.fit(X_train_scaled, y_train)
y_pred_xgb = xgb.predict(X_test_scaled)

"""## **Model LightGBM**"""

lgb = LGBMRegressor(random_state=42)
lgb.fit(X_train_scaled, y_train)
y_pred_lgb = lgb.predict(X_test_scaled)

"""## **Model Seasonal ARIMA**"""

# Baseline Model: SARIMA (harian)
traffic_ts = traffic[['traffic_volume']].copy()
traffic_daily = traffic_ts.resample('D').mean().fillna(method='ffill')

# Split SARIMA train-test
train_sarima = traffic_daily[:'2017']
test_sarima = traffic_daily.loc['2018']

# SARIMAX Model
exog_features = ['temp', 'rain_1h', 'snow_1h']
exog = traffic[exog_features].resample('D').mean().fillna(method='ffill')
exog_train = exog.loc[train_sarima.index]
exog_test = exog.loc[test_sarima.index]

sarimax_model = SARIMAX(train_sarima, exog=exog_train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7))
sarimax_fit = sarimax_model.fit()
sarimax_pred = sarimax_fit.forecast(steps=len(test_sarima), exog=exog_test)

"""# **Evaluasi Model**"""

# Fungsi evaluasi model dengan rmse dan mae
def evaluate_model(name, y_true_train, y_pred_train, y_true_test, y_pred_test):
    return {
        'model': name,
        'RMSE_train': np.sqrt(mean_squared_error(y_true_train, y_pred_train)),
        'MAE_train': mean_absolute_error(y_true_train, y_pred_train),
        'RMSE_test': np.sqrt(mean_squared_error(y_true_test, y_pred_test)),
        'MAE_test': mean_absolute_error(y_true_test, y_pred_test)
    }

# Simpan hasil evaluasi
results = []

from xgboost import XGBRegressor
import lightgbm as lgbm
from statsmodels.tsa.statespace.sarimax import SARIMAX

results = []

# XGBoost
y_pred_train_xgb = xgb.predict(X_train_scaled)
y_pred_test_xgb = xgb.predict(X_test_scaled)
results.append(evaluate_model('XGBoost', y_train, y_pred_train_xgb, y_test, y_pred_test_xgb))

# LightGBM
y_pred_train_lgb = lgb.predict(X_train_scaled)
y_pred_test_lgb = lgb.predict(X_test_scaled)
results.append(evaluate_model('LightGBM', y_train, y_pred_train_lgb, y_test, y_pred_test_lgb))

# SARIMAX
y_pred_train_sarima = sarimax_fit.predict(start=train_sarima.index[0], end=train_sarima.index[-1], exog=exog_train)
y_pred_test_sarima = sarimax_fit.forecast(steps=len(test_sarima), exog=exog_test)
results.append(evaluate_model('SARIMAX', train_sarima, y_pred_train_sarima, test_sarima, y_pred_test_sarima))

# Tabel Evaluasi
eval_df = pd.DataFrame(results).set_index('model')
eval_df = eval_df.round(2)  # Bulatkan biar rapi
eval_df

# Buat DataFrame MSE
mse = pd.DataFrame(columns=['train', 'test'], index=['XGBoost', 'LightGBM'])

# Buat dictionary model
model_dict = {
    'XGBoost': xgb,
    'LightGBM': lgb
}

# Hitung MSE per model
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_train, model.predict(X_train_scaled)) / 1e3
    mse.loc[name, 'test'] = mean_squared_error(y_test, model.predict(X_test_scaled)) / 1e3

# Tambah SARIMA ke dictionary model manual (karena perlu prediksi beda cara)
mse.loc['SARIMA', 'train'] = mean_squared_error(
    train_sarima,
    sarimax_fit.predict(start=train_sarima.index[0], end=train_sarima.index[-1], exog=exog_train)
) / 1e3

mse.loc['SARIMA', 'test'] = mean_squared_error(
    test_sarima,
    sarimax_fit.predict(start=test_sarima.index[0], end=test_sarima.index[-1], exog=exog_test)
) / 1e3

# Panggil MSE
mse

# Plot vertikal
ax = mse.plot(kind='bar', figsize=(8, 5), color=['skyblue', 'salmon'], rot=0)
plt.title('Perbandingan MSE pada Data Train vs Test per Model')
plt.ylabel('MSE')
plt.xlabel('Model')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.legend(title='Dataset')
plt.tight_layout()
plt.show()

"""**Insight**: XGBoost memiliki nilai error paling kecil, maka dipilih sebagai model terbaik untuk memprediksi volume lalu lintas"""

# Menguji Prediksi
prediksi = X_test.iloc[:1].copy()
pred_dict = {'y_true': y_test[:1]}
for name, model in model_dict.items():
    pred_dict['prediksi_' + name] = model.predict(prediksi).round()

# Menambahkan SARIMA
pred_date = prediksi.index[0].floor('D')
sarimax_pred_value = sarimax_fit.predict(start=pred_date, end=pred_date, exog=exog.loc[[pred_date]])
pred_dict['prediksi_SARIMAX'] = sarimax_pred_value.values.round()

tabel = pd.DataFrame(pred_dict)
display(tabel)

"""**Insight**: Model XGBoost memberi prediksi yang paling mendekati nilai sesungguhnya"""